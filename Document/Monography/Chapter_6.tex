\begin{appendices}
\chapter{Z-score Standardization}\label{ap:zscore}
In the process of calculate the four order polynomial fit the function np.poly.fit() presents an overstimation on the coefficients, due to the large difference of magnitude order between axis.
To deal with this difference a z-score standardization was used on the selected bins of wavelengths around the observed wavelength. 
This process helps to avoid the dominance of certain features over other due to diferences in their scales \cite{Boyd_2014}.

The follow up for the standardization was applied the relation \eqref{z score} on the selected bins for wavelength.

\begin{equation}
\lambda_{scaled}= \frac{\lambda-\mu(\lambda)}{\sigma(\lambda)}
\label{z score}
\end{equation}

Where $\mu(\lambda)$ refers to the mean and $\sigma(\lambda)$ to the standard deviation of the wavelength range. 
As the wavelength was scaled, in terms of calculated derivates for the first and the second signature, a re-scaled for this values was necessary.
Based on the definition for the standardization, the derivates follow the relation \eqref{re scaled derivates}.

\begin{equation}
 \frac{d}{d \lambda} = \frac{1}{\sigma(\lambda)} \frac{d}{d \lambda_{scaled}}
\label{re scaled derivates}
\end{equation}

Taking the derivate of the expresion \eqref{z score} a factor related to the standard deviation appear.
With this, the original values for derivates evaluated in the observed wavelength are expresed in equation \eqref{2 and 3 scaled derivate}

\begin{equation}
 \frac{d^2}{d \lambda^2} = \frac{1}{\sigma(\lambda)^2} \frac{d^2}{d \lambda_{scaled}^2} \quad \quad \frac{d^3}{d \lambda^3} = \frac{1}{\sigma(\lambda)^3} \frac{d^3}{d \lambda_{scaled}^3}
\label{2 and 3 scaled derivate}
\end{equation}

This improved considerably the precision in the fit and there over the precision on the observed wavelength calculated.

\chapter{The third derivate relation}\label{ap:third derivate}

Or called the bisector slope. 
It was multiplied by the relation ($\frac{c}{\lambda}$) to see each clear in the graphic.

\chapter{Visualizer for outliers}\label{ap:visualizer}

For the process of the blend-free list was created an app using the interface Tkinter with the objective to help the visualization of outliers.
Two versions of the visualizer were created.
One just shows the line core and fourth order polynomial fit as shown in the figure ().

This helps for a first process where far separated lines were discarded.
Then, we can perform the different calculations (core curvature, velocity and bisector slope), and use the second version of the visualizer (see figure ()).

In this version it can visualize the three signatures of convection and the line core with the fit. Moreover, was resalted the corresponding Fe I line on each graph to corroborate the behavior.
Thanks to this software the time expended seeing lines was reduced significantly. Specially cause count with his own system to classification, adding lines to a Dataframe and save the image. Following the motivation we present the software on GitHub \href{https://github.com/ccuellarn/Final-Project}{Repository on GitHub where was published the visualizer of mixed lines.} and its explained below.

\section{Test example}

The main code is in the file Visualizer.ipynb and the test example data are test.xlsx, feel free to change the type of data, the important is make a dataframe where the columns are [Wave , Flux] wavelength on Armstrong and flux normalized preferred. Then made another dataframe with the list of lines of Fe I.

The function closer lines select the closer minimums of the FeI lines and save the wave flux of that minimum point and the Fe I line associated. Don't be confused, this is not the observed wavelength. This point is a reference for selecting the bins around the Fe I line. The function discards distances over 0.001A.

Then the function local points select the bins of each corresponding to one index on the closer lines dataframe. Each bin of wavelength is for 0.1mA around the minimum point. 

The function Derivatives find the polynomial fourth order fit and calculate the minimum point with the fit, that is the observed wavelength. This returns a dataframe with the FeI line, flux fit and the minimum observed.

In parallel are calculated the bisectors of each line following the midpoint method, where equal points of flux are selected for comparison.

From this is the first visualizer that receives the local points, the values and the fit. This shows the line core and the fourth polynomial fit, in parallel is shown the bisector of each line in terms of velocity.

We recommend eliminating the lines that follow one of the conditions presented below:
1)The bisector doesn't show a C-curved bisector or it's too affected by the noise.
2)There is no curve or polynomial fit. This can be interpreted as the position on other points to the fit.
3) There's too much noise on the original line.

With this first filter the number of possible lines are reduced for calculating derivatives.

The second part of the code calculates the granulation pattern, core curvature and bisector slope. With these values the visualizer shows all the graphics including the line profile with the polynomial fit. In each graphic of derivatives the corresponding Fe I is resalted, this with the finally to select lines depending on his behavior.
 
This is a code test:
Run the file test, you can adapt this part on your necessaries. The idea is the Dataframe results have the columns Wave (cm), nFlux and Wave A (there's no need for the flux to be normalized, it cannot be, we test this on arturus). 

Then run the nave list, the present github has the table organized on an excel.

Run the cell of closer points and local points, the first needs to be returned a Dataframe and the other a list of dataframes.

For the first filter code you need to have a dataframe with the columns (), the list of local points and the closer lines associated with the Dataframe, the fit values and the covariance values.

Modified this line to call the first visualizer.

Then, this part helps to extract the Dataframe with the lines that don't behave like the condition parameters. This can also help to drop the unnecessary lines

For the second enter the file with the lines resulting in the first filter, and run again the code for closer lines and local points, and then the second big filter.

Modified this line to call the second visualizer.

The next line helps to extract the lines to drop and remove it for the Dataframe.

\section{Conditions justifications}
Two is for observations on polynomial fits that derive for mathematics properties. The third born on the many observations that i realize after the creation of visualizer, I see the parameter of 0.001 for near lines and I put the value

\end{appendices}
